1. FINDING JS FILES------------------------------------------------------

## ‚úÖ Using Crawled URLs + `grep`

cat all_urls.txt | grep -Eo 'https?://[^ ]+\.js([?#][^ ]*)?' | sort -u | anew crawled_jsfiles.txt


## ‚úÖ Using Katana

katana -list live_subdomains.txt -jc -d 5 -o katana_all.txt
 grep -Eo 'https?://[^ ]+\.js([?#][^ ]*)?' katana_all.txt| sort -u > katana_jsfiles.txt



## ‚úÖ Using `subjs`

cat live_subdomains.txt | subjs >>subjs_jsfiles.txt
cat allurls.txt | subjs | sort -u >> subjs_jsfiles.txt


##COMBINING ALL FILES:
cat crawled_jsfiles.txt katana_jsfiles.txt subjs_jsfiles.txt | sort -u >>alljs.txt

##FILTERING:
httpx -mc 200 -silent -o livejs.txt < alljs.txt



------------------------------------------------------------------------------



2. DOWNLOADING JS FILES

cat livejs.txt | xargs -I {} wget --content-disposition -q -P js_downloads {}


------------------------------------------------------------------------------

# üïµÔ∏è‚Äç‚ôÇÔ∏è MANUAL REVIEW & STATIC ANALYSIS

## üìö Tools:

---

1.Using linkfinder:

mkdir linkfinder_results

while read domain; do   echo "[+] Scanning $domain";   python3 /home/maddy/techiee/bug_bounty/2_phase_recon_autom/recon/js_recon/LinkFinder/linkfinder.py -i "$domain" -d -o cli | tee "linkfinder_results/$(echo $domain | sed 's|https\?://||;s|/|_|g')_results.txt"; done < oi 

while read domain; do   echo "[+] Scan2ing $domain";   output_file="linkfinder_results/$(echo $domain | sed 's|https\?://||;s|/|_|g').html";   python3 /home/maddy/techiee/bug_bounty/2_phase_recon_autom/recon/js_recon/LinkFinder/linkfinder.py -i "$domain" -d -o "$output_file"; done < domains.txt

python3 /home/maddy/techiee/bug_bounty/2_phase_recon_autom/recon/js_recon/LinkFinder/linkfinder.py -i './js_downloads/*' -o cli | tee linkfinder_results/linkfinder_cli_results_jscontents
python3 /home/maddy/techiee/bug_bounty/2_phase_recon_autom/recon/js_recon/LinkFinder/linkfinder.py -i './js_downloads/*' -o linkfinder_results/linkfinder_html_results_jscontents


Look for:

    Endpoints (/api/user/update, /admin/config)
    API Keys, secrets, tokens
    JWT endpoints, S3 buckets

    ‚úÖ Secret Trick: Run regex searches on downloaded JS files to find auth headers, tokens, and debug endpoints.


## 2. Token/Key Extraction (Manual grep)


cat js_downloads/* | grep -Eo "/api/[a-zA-Z0-9_/-]+" | sort -u >>api_endpoints
```
long alphanumeric dtrings:
cat js_downloads/* | grep -EHo '([A-Za-z0-9_]{15,})' | tee strings.txt

basic auth tokens:
cat js_downloads/* | grep -Ei 'Basic[\s\-_A-Za-z0-9]*[:=][\s\-_A-Za-z0-9]{10,}' | tee auth_tokens.txt

sens keyword:
grep -Poir --exclude='*.min.js' --binary-files=without-match   -e '.{0,15}(api[_-]?key|aws_access_key|innertext|inner|internal|localhost|aws_secret_key|api key|passwd|pwd|heroku|slack|firebase|swagger|aws_secret_key|aws key|password|ftp password|jdbc|db|sql|secret jet|config|admin|pwd|json|gcp|htaccess|.env|ssh key|.git|access key|secret token|oauth_token|oauth_token_secret|secret|token|fetch|axios|debug|eval|internal|authorization|env|config|bearer|client[_-]?id|client[_-]?secret|jwt|admin|pass(word)?|cred(entials)?).{0,15}'   js_downloads/ | sed -E 's/(api[_-]?key|secret|token|authorization|bearer|client[_-]?id|client[_-]?secret|jwt|admin|pass(word)?|cred(entials)?)/\x1b[31m\1\x1b[0m/Ig' > sens_keys_short.txt




## 3Ô∏è‚É£ `SecretFinder`


mkdir secretfinder_results

python3 /home/maddy/techiee/bug_bounty/2_phase_recon_autom/recon/js_recon/SecretFinder/SecretFinder.py -i 'js_downloads/*' -o cli | tee secretfinder_results/results.txt


#html output:
while read -r url; do
  python3 /home/maddy/techiee/bug_bounty/2_phase_recon_autom/recon/js_recon/SecretFinder/SecretFinder.py -i "$url" -e -o "output_$(echo $url | sed 's~https\?://~~; s~/~-~g').html"
done < urls.txt

#cli,text output
while read -r url; do
  outname="output_$(echo $url | sed 's~https\?://~~; s~/~-~g').txt"
  python3 /home/maddy/techiee/bug_bounty/2_phase_recon_autom/recon/js_recon/SecretFinder/SecretFinder.py -i "$url" -e -o cli | tee "$outname"
done < urls.txt


## 4Ô∏è‚É£ `mantra`

mkdir /mantra
cat livejs.txt | mantra | tee mantra/mantra_results.txt


## 5Ô∏è‚É£ `lazyegg`
mkdir lazyegg

while read -r url; do   domain=$(echo "$url" | sed -E 's~https?://([^/]+).*~\1~');   echo -e "\n========== Target: $domain ==========\n" | tee -a output.txt;   python3 lazyegg.py "$url" | tee -a output.txt_domains.txt; done < live



scanning multiple js:
cat livejs.txt | xargs -I{} bash -c 'echo -e "\n[+] Target: {}\n" | tee -a js_results.txtt && python3 lazyegg.py "{}" --js_urls --domains --ips --leaked_creds | tee -a output.txt_jsfiles'


## 6Ô∏è‚É£ `nuclei` JS Exposure Detection

mkdir nuclei-res/

nuclei -l livejs.txt -t ~/nuclei-templates/http/exposures -o  nuclei-res/js_quick_exposures_results.txt
nuclei -l livejs.txt   -t ~/nuclei-templates/   -severity low,medium,high,critical -o nuclei-res   -rate-limit 150   -stats



## 7Ô∏è‚É£ S3 Bucket Takeover

mkdir s3/
# Find S3 bucket URLs
cat livejs.txt | xargs -I% curl -sk "%" | grep -Eo '([a-z0-9\.-]+)\.s3.*\.amazonaws\.com' >> s3_bucket.txt

# Extract bucket names
cat s3_bucket.txt | sed 's/.*s3.*\.amazonaws\.com\///' | sort -u > bucket_name.txt


## 8Ô∏è‚É£ JSFScan 

bash JSFScan.sh -l live_subdomains.txt --all -r -o jsfscan_results




---

# üíª Burp for Deep Analysis

* Use JSFinder + JSLuice++ extensions.
* Analyze request/response structure.
* Discover endpoints, tokens, and cookie manipulation.
## ‚úÖ Using Burp Suite

* Use HTTP history and sitemap to filter JS.
* Use **JSFinder** & **jsluice++** Burp extensions.


* Use **[de4js](https://lelinhtinh.github.io/de4js/)** to beautify/obfuscate JS.
* Ask ChatGPT for interpreting suspicious code blocks or endpoints.

